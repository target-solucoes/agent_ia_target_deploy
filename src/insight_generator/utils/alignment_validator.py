"""
Insight Alignment Validator - FASE 5: Output Alignment Validation

This module validates alignment between:
- narrative text
- detailed_insights structure
- composed metrics
- metadata

Ensures that every metric mentioned in the narrative is:
1. Present in the composed metrics
2. Detailed in detailed_insights with explicit formula
3. Numerically consistent across sections

Reference: insight_generator_planning.md - Section 7.2
"""

import re
import logging
from typing import Dict, Any, List, Tuple, Optional

logger = logging.getLogger(__name__)


class InsightAlignmentValidator:
    """
    Validates alignment between narrative and detailed_insights.

    Ensures semantic consistency and traceability of all metrics
    mentioned in the generated insights.
    """

    def __init__(self):
        """Initialize validator with metric patterns."""
        # Patterns for extracting numeric values and metric references
        self.numeric_pattern = re.compile(
            r"(?:R\$\s*)?(\d+[.,]\d+)\s*([MKmkBb%])?", re.IGNORECASE
        )

        # Common metric keywords to identify references
        self.metric_keywords = {
            "top",
            "gap",
            "concentração",
            "concentracao",
            "variação",
            "variacao",
            "crescimento",
            "queda",
            "delta",
            "média",
            "media",
            "total",
            "participação",
            "participacao",
            "share",
            "percentual",
            "taxa",
            "amplitude",
            "tendência",
            "tendencia",
            "volatilidade",
            "líder",
            "lider",
            "segundo",
            "terceiro",
        }

    def validate(
        self,
        narrative: str,
        detailed_insights: List[Dict[str, Any]],
        composed_metrics: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        Main validation method.

        Args:
            narrative: The narrative text generated by LLM
            detailed_insights: List of detailed insight dicts with formulas
            composed_metrics: Optional dict of metrics from MetricComposer

        Returns:
            Dict with:
                - is_aligned: bool
                - alignment_score: float (0.0 to 1.0)
                - missing_in_detailed: List[str] - metrics in narrative but not in detailed
                - missing_in_narrative: List[str] - metrics in detailed but not in narrative
                - value_mismatches: List[Dict] - numeric inconsistencies
                - warnings: List[str]
        """
        logger.info("[AlignmentValidator] Starting alignment validation")

        result = {
            "is_aligned": True,
            "alignment_score": 1.0,
            "missing_in_detailed": [],
            "missing_in_narrative": [],
            "value_mismatches": [],
            "warnings": [],
        }

        if not narrative or not detailed_insights:
            result["is_aligned"] = False
            result["alignment_score"] = 0.0
            result["warnings"].append("Empty narrative or detailed_insights")
            return result

        # Extract metrics mentioned in narrative
        narrative_metrics = self._extract_narrative_metrics(narrative)
        logger.info(
            f"[AlignmentValidator] Found {len(narrative_metrics)} metric references in narrative"
        )

        # Extract metrics from detailed_insights
        detailed_metrics = self._extract_detailed_metrics(detailed_insights)
        logger.info(
            f"[AlignmentValidator] Found {len(detailed_metrics)} metrics in detailed_insights"
        )

        # Check for missing metrics in detailed_insights
        missing_in_detailed = []
        for metric_ref in narrative_metrics:
            if not self._is_metric_in_detailed(metric_ref, detailed_metrics):
                missing_in_detailed.append(metric_ref)

        if missing_in_detailed:
            result["missing_in_detailed"] = missing_in_detailed
            result["warnings"].append(
                f"Found {len(missing_in_detailed)} metric(s) in narrative without detailed explanation"
            )

        # Check for value consistency
        value_mismatches = self._check_value_consistency(narrative, detailed_insights)

        if value_mismatches:
            result["value_mismatches"] = value_mismatches
            result["warnings"].append(
                f"Found {len(value_mismatches)} numeric value mismatch(es)"
            )

        # Calculate alignment score
        total_checks = len(narrative_metrics) + len(value_mismatches)
        if total_checks > 0:
            issues = len(missing_in_detailed) + len(value_mismatches)
            result["alignment_score"] = max(0.0, 1.0 - (issues / total_checks))

        # Determine overall alignment status
        # Aligned if score >= 0.95 (allowing 5% tolerance)
        result["is_aligned"] = result["alignment_score"] >= 0.95

        logger.info(
            f"[AlignmentValidator] Validation complete: "
            f"aligned={result['is_aligned']}, score={result['alignment_score']:.2f}"
        )

        return result

    def _extract_narrative_metrics(self, narrative: str) -> List[Dict[str, Any]]:
        """
        Extract metric references from narrative text.

        Returns list of dicts with:
            - type: str (e.g., "concentration", "gap", "variation")
            - value: Optional[str] (numeric value if present)
            - context: str (surrounding text)
        """
        metrics = []

        # Split narrative into sentences for context
        sentences = re.split(r"[.!?]\s+", narrative)

        for sentence in sentences:
            sentence_lower = sentence.lower()

            # Check for metric keywords
            for keyword in self.metric_keywords:
                if keyword in sentence_lower:
                    # Extract numeric values in this sentence
                    values = self.numeric_pattern.findall(sentence)

                    metric_ref = {
                        "type": keyword,
                        "values": [self._normalize_value(v) for v in values],
                        "context": sentence[:100],  # First 100 chars for context
                    }
                    metrics.append(metric_ref)

        return metrics

    def _extract_detailed_metrics(
        self, detailed_insights: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Extract metrics from detailed_insights structure.

        Each insight should have:
            - title or metric_name
            - formula
            - value
            - interpretation (optional)
        """
        metrics = []

        for insight in detailed_insights:
            # Support multiple field name conventions
            metric_name = (
                insight.get("metric_name")
                or insight.get("title")
                or insight.get("metric")
                or "unnamed_metric"
            )

            formula = insight.get("formula", "")
            value = insight.get("value", "")

            # Extract numeric values from formula and value
            formula_values = self.numeric_pattern.findall(formula)
            value_values = self.numeric_pattern.findall(str(value))

            metrics.append(
                {
                    "name": metric_name,
                    "formula": formula,
                    "value": value,
                    "numeric_values": [
                        self._normalize_value(v)
                        for v in (formula_values + value_values)
                    ],
                }
            )

        return metrics

    def _is_metric_in_detailed(
        self, metric_ref: Dict[str, Any], detailed_metrics: List[Dict[str, Any]]
    ) -> bool:
        """
        Check if a metric reference from narrative exists in detailed_insights.

        Uses fuzzy matching on metric type and values.
        """
        ref_type = metric_ref["type"]
        ref_values = set(metric_ref["values"])

        for detailed in detailed_metrics:
            # Check if metric name/title contains the reference type
            name_lower = detailed["name"].lower()
            if ref_type in name_lower:
                # If values are present, check if any match
                if not ref_values:
                    return True  # Type match is enough if no specific values

                detailed_values = set(detailed["numeric_values"])
                if ref_values & detailed_values:  # Intersection
                    return True

        return False

    def _check_value_consistency(
        self, narrative: str, detailed_insights: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Check for numeric value consistency between narrative and detailed_insights.

        Returns list of mismatches with:
            - value: str
            - found_in_narrative: bool
            - found_in_detailed: bool
            - context: str
        """
        mismatches = []

        # Extract all numeric values from narrative
        narrative_values = set(
            [self._normalize_value(v) for v in self.numeric_pattern.findall(narrative)]
        )

        # Extract all numeric values from detailed_insights
        detailed_values = set()
        for insight in detailed_insights:
            formula = insight.get("formula", "")
            value = insight.get("value", "")

            formula_nums = self.numeric_pattern.findall(formula)
            value_nums = self.numeric_pattern.findall(str(value))

            for v in formula_nums + value_nums:
                detailed_values.add(self._normalize_value(v))

        # Check for values in narrative not in detailed
        missing_values = narrative_values - detailed_values

        # Only report significant mismatches (values mentioned explicitly)
        # Small differences due to rounding are acceptable
        for value in missing_values:
            # Skip very common generic values (100, 0, etc)
            if value in ["0", "100", "1"]:
                continue

            mismatches.append(
                {
                    "value": value,
                    "found_in_narrative": True,
                    "found_in_detailed": False,
                    "message": f"Value {value} mentioned in narrative but not in detailed_insights",
                }
            )

        return mismatches

    def _normalize_value(self, value_tuple: Tuple[str, str]) -> str:
        """
        Normalize numeric value for comparison.

        Args:
            value_tuple: (number, unit) from regex match

        Returns:
            Normalized string for comparison
        """
        num_str, unit = value_tuple

        # Normalize decimal separator
        num_str = num_str.replace(",", ".")

        # Round to 2 decimal places for comparison
        try:
            num = float(num_str)
            num_str = f"{num:.2f}"
        except ValueError:
            pass

        # Normalize unit
        unit = unit.upper() if unit else ""

        return f"{num_str}{unit}"


def validate_alignment(
    narrative: str,
    detailed_insights: List[Dict[str, Any]],
    composed_metrics: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """
    Convenience function for alignment validation.

    Args:
        narrative: The narrative text
        detailed_insights: List of detailed insight dicts
        composed_metrics: Optional metrics from MetricComposer

    Returns:
        Validation result dict
    """
    validator = InsightAlignmentValidator()
    return validator.validate(narrative, detailed_insights, composed_metrics)
